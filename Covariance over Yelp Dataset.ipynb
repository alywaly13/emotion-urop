{
 "metadata": {
  "name": "",
  "signature": "sha256:b2795bd9fb8b55fffc8db257d98b749df8fcac9e2e22d76c106d7d06f97944e2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext cythonmagic\n",
      "%load_ext autoreload\n",
      "%matplotlib inline\n",
      "%autoreload 2\n",
      "import numpy as np, os, sys, gzip, pickle, re\n",
      "import matplotlib.pyplot as plt\n",
      "%config InlineBackend.figure_format = 'png'\n",
      "from collections import OrderedDict\n",
      "sys.path.append(\"/home/alice/Urop/\")\n",
      "#sys.path.append(\"/usr/local/\")\n",
      "#sys.path.append(\"/usr/lib/\")\n",
      "#sys.path.append(\"/usr/lib/python2.7/dist-packages/pandas/\")\n",
      "#sys.path.append(\"/usr/local/python_packages/yaafelib/\")\n",
      "from word2vec_extended import Word2VecExtended\n",
      "from IPython.display import clear_output\n",
      "from utils import present_restaurant, get_adjacency_matrix, get_degree_matrix, assign_parents\n",
      "from IPython.display import display, HTML\n",
      "from IPython.html.widgets import interact, fixed, IntSliderWidget\n",
      "from scipy.cluster.hierarchy import linkage, to_tree, dendrogram\n",
      "from tsne import bh_sne\n",
      "from objectlm import ObjectLM, CategoriesConverter, DatasetGenerator, HierarchicalObservation\n",
      "lmsenti = Word2VecExtended.load(\"/home/alice/Urop/kid_model_30_oov_senti\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The cythonmagic extension is already loaded. To reload it, use:\n",
        "  %reload_ext cythonmagic\n",
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time \n",
      "#os.system(\"./yelp.py\")\n",
      "#Start ./yelp.py from terminal in the folder that contains this notebook. This must be running for this function to work. \n",
      "def doEmoStuff(restaurant, wavFile):\n",
      "    ##restaurant is the number corresponding to the restaurant\n",
      "    ##wavFile is the path/name of the wavFile of the sound sample. \n",
      "    ##Ex: doEmoStuff(1334, \"/user/alice/wavs/a05.wav\")\n",
      "    ##    This calls \"makeWeightedObs with the appropriate weight (and for now also prints the weight)\n",
      "    cmd = \"cp \" + os.path.join(\"../\", wavFile) + \" ./sentWavFile.wav\"\n",
      "    os.system(cmd)\n",
      "    gotAns=False\n",
      "    while not gotAns:\n",
      "        gotAns=os.path.isfile(\"emoNumber.txt\")\n",
      "    time.sleep(0.01)\n",
      "    f=open(\"emoNumber.txt\", \"r\")\n",
      "    emoNum=f.read()\n",
      "    print(emoNum)\n",
      "    \n",
      "    f.close()\n",
      "    os.system(\"rm emoNumber.txt\")\n",
      "    make_weighted_observations(restaurant, [], np.array([emoNum]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aaa\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file = gzip.open(\"saves/saved_texts.gz\", 'r')\n",
      "texts, texts_data = pickle.load(file)\n",
      "file.close()\n",
      "categories = set()\n",
      "for el in texts_data:\n",
      "    for c in el[\"categories\"]:\n",
      "        categories.add(c)\n",
      "catconvert = CategoriesConverter(categories)\n",
      "dataset_gen = DatasetGenerator(texts, texts_data, catconvert)\n",
      "\n",
      "# should modify this to do some auto-encoding / self regression.\n",
      "model = ObjectLM(\n",
      "    vocabulary = lmsenti,\n",
      "    object_vocabulary_size = len(texts),\n",
      "    window = 10,\n",
      "    bilinear_form = False,\n",
      "    size = 20,\n",
      "    object_size = 20,\n",
      "    output_sigmoid_classes = catconvert.num_categories,\n",
      "    output_sigmoid_labels = catconvert.index2category,\n",
      "    output_labels = [[\"\", \"$\", \"$$\", \"$$$\", \"$$$$\"], [\"1\", \"2\", \"3\", \"4\", \"5\"]],\n",
      "    output_classes=[5, 5] # \"\", \"$\", \"$$\",...,\"$$$$\", 5 price classes, and 5 rating classes\n",
      ")\n",
      "model.load_saved_weights(\"saves/objectlm_window_10_lm_20_objlm_20_4/\")\n",
      "model.create_normalized_matrices()\n",
      "\n",
      "\n",
      "if os.path.exists(\"saves/__linkage_average.npy\"):\n",
      "    Z = np.load(\"saves/__linkage_average.npy\")\n",
      "else:\n",
      "    y = model.object_matrix.get_value(borrow=True)\n",
      "    Z = linkage(y, method='average', metric='cosine')\n",
      "    np.save(\"saves/__linkage_average.npy\", Z)\n",
      "    \n",
      "    \n",
      "if os.path.exists(\"saves/__covariance__.npy\"):\n",
      "    Cov = np.load(\"saves/__covariance__.npy\")\n",
      "    observables = HierarchicalObservation(Cov)\n",
      "else:\n",
      "    root, nodes = to_tree(Z, rd=True)\n",
      "    assign_parents(root)\n",
      "    adj_mat = get_adjacency_matrix(nodes_small)\n",
      "    deg_mat = get_degree_matrix(nodes_small)\n",
      "    sigma = 5\n",
      "    laplacian = np.diag(deg_mat) - adj_mat + 1/(sigma**2) * np.eye(len(deg_mat))\n",
      "    Cov = np.linalg.inv(laplacian)[0:model.object_vocabulary_size,0:model.object_vocabulary_size]\n",
      "    np.save(\"saves/__covariance__.npy\", Cov)\n",
      "    observables = HierarchicalObservation(Cov)\n",
      "    \n",
      "    \n",
      "from pyxdameraulevenshtein import damerau_levenshtein_distance as levenshtein_distance\n",
      "\n",
      "def search_for_object(self, object_index, topn = 10, metric = 'cosine'):\n",
      "    present_restaurant(texts_data[object_index], text = texts[object_index])\n",
      "    display(HTML(\"<small>%d</small>\" % object_index))\n",
      "    \n",
      "    print(object_index)\n",
      "    \n",
      "    results = self.most_similar_object(object_index, topn=topn, metric = metric)\n",
      "    if metric == 'euclidean':\n",
      "        max_distance = max(result[1] for result in results)\n",
      "    else:\n",
      "        max_distance = 1\n",
      "    \n",
      "    for result, distance in results:\n",
      "        present_restaurant(texts_data[result], text = texts[result])\n",
      "        display(HTML(\"<small>%d</small>\" % result))\n",
      "        display(HTML(\"\"\"\n",
      "            <div>\n",
      "                <div style='background: rgb(221,222,223);width:102px;padding:1px;border-radius:2px'>\n",
      "                    <div style='height:20px; width:%dpx;padding-left:3px;background-color:rgb(71, 189, 249);font-size:9px;color:white'>\n",
      "                        %.0f%%\n",
      "                    </div>\n",
      "                </div>\n",
      "            </div>\n",
      "            \"\"\" % (int(distance * 100 / max_distance), distance * 100 / max_distance)))\n",
      "        \n",
      "def nearest_name(text, levenshtein = True):\n",
      "    text = text.lower()\n",
      "    min_distance_index = -1\n",
      "    min_distance = float('inf')\n",
      "    min_distance_word = None\n",
      "    \n",
      "    for i, datum in enumerate(texts_data):\n",
      "        if levenshtein:\n",
      "            min_local_distance = float('inf')\n",
      "            if datum[\"_id\"].lower().find(text) != -1:\n",
      "                min_local_distance = 3\n",
      "                if min_local_distance < min_distance:\n",
      "                    min_distance_word = datum[\"_id\"]\n",
      "                    min_distance = min_local_distance\n",
      "                    min_distance_index = i\n",
      "            for scrap in re.split( \"[ -]\", datum[\"_id\"].lower()) + [datum[\"_id\"].lower()] + datum[\"categories\"]:\n",
      "                min_local_distance = min(min_local_distance, levenshtein_distance(text, scrap))\n",
      "                if min_local_distance < min_distance:\n",
      "                    min_distance_word = scrap\n",
      "                    min_distance = min_local_distance\n",
      "                    min_distance_index = i\n",
      "            if min_distance <= 1:\n",
      "                break\n",
      "        else:\n",
      "            if datum[\"_id\"].lower().find(text) != -1 or datum[\"id\"].lower().find(text) != -1:\n",
      "                min_distance = i\n",
      "                break\n",
      "            for cat in datum[\"categories\"]:\n",
      "                if cat.lower().find(text) != -1:\n",
      "                    min_distance = i\n",
      "                    break\n",
      "    \n",
      "    \n",
      "    if min_distance_index == -1 or min_distance == len(text) or min_distance == len(min_distance_word):\n",
      "        return (None, None, None)\n",
      "    else:\n",
      "        return (min_distance_word, min_distance_index, min_distance)\n",
      "\n",
      "def search_with_text(self, text, topn = 10, levenshtein = True, metric = 'cosine'):\n",
      "    min_distance_word, min_distance_index, min_distance = nearest_name(text, levenshtein=levenshtein)\n",
      "    \n",
      "    if min_distance_index == None:\n",
      "        print(\"Could not be found\")\n",
      "    else:\n",
      "        if min_distance > 0:\n",
      "            display(HTML(\"\"\"\n",
      "            <span style=\"color: #333\">Did you mean </span> <b>%s</b> <span style=\"color: #333\">(%d edit%s)</span> ?\n",
      "            \"\"\" % (min_distance_word, min_distance, \"s\" if min_distance != 1 else \"\")))\n",
      "        return search_for_object(self, min_distance_index, topn=topn, metric = metric)\n",
      "    \n",
      "INDEX_SET = set(range(0,model.object_vocabulary_size))\n",
      "\n",
      "def show_conditional_probs(query=\"thai\", topn=20, testn=2):\n",
      "    min_distance_word, min_distance_index, min_distance = nearest_name(query)\n",
      "    test_indices = [min_distance_index] + [i[0] for i in model.most_similar_object(min_distance_index)][:testn-1]\n",
      "    result_inds, result_probs = observables.conditional_probabilities(test_indices, remaining_indices = list(INDEX_SET - set(test_indices)))\n",
      "\n",
      "    top_results = np.argsort(result_probs[:,0])[::-1][:topn]\n",
      "\n",
      "    for i in range(topn):\n",
      "        present_restaurant(texts_data[result_inds[top_results[i]]], text=texts[result_inds[top_results[i]]])\n",
      "        print(result_probs[top_results[i],0])\n",
      "        \n",
      "def show_conditional_probs_from_index(indices, topn=20):\n",
      "    result_inds, result_probs = observables.conditional_probabilities(indices, remaining_indices = list(INDEX_SET - set(indices)))\n",
      "\n",
      "    top_results = np.argsort(result_probs[:,0])[::-1][:topn]\n",
      "\n",
      "    for i in range(topn):\n",
      "        present_restaurant(texts_data[result_inds[top_results[i]]], text=texts[result_inds[top_results[i]]])\n",
      "        print(result_probs[top_results[i],0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'CategoriesConverter' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-34-8097478d3edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"categories\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mcategories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcatconvert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategoriesConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdataset_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'CategoriesConverter' is not defined"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar_word(\"spicy\", topn = 10, metric='euclidean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar_word(\"bland\", topn = 10, metric='euclidean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar_word(\"cancer\", topn = 10, metric='euclidean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar_word(\"sandwich\", topn = 10, metric='euclidean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar_word(\"sandwich\", topn = 10, metric='cosine')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interact(search_with_text, self=fixed(model), levenshtein_distance=fixed(True), text=\"Tangerine Thai\", metric=('cosine', 'euclidean'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interact(show_conditional_probs,\n",
      "         topn=IntSliderWidget(min=1, max=100, value = 20),\n",
      "         testn=IntSliderWidget(min=1, max=100, value=3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from tsne import bh_sne"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datum = bh_sne(model.object_matrix.get_value(borrow=True).astype(np.float64), d=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datum.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(datum[:,0], datum[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datum2 = bh_sne(model.object_matrix.get_value(borrow=True).astype(np.float64), d=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(datum2[:,0], datum2[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datum3 = bh_sne(model.norm_object_matrix.astype(np.float64), d=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sort_data(dataset, labels):\n",
      "    buckets = [[] for i in range(len(labels) + 1)]\n",
      "    assigned = False\n",
      "    for i in range(len(dataset)):\n",
      "        assigned = False\n",
      "        datacats = texts_data[i][\"categories\"]\n",
      "        for k, cat in enumerate(labels):\n",
      "            if cat in datacats:\n",
      "                buckets[k+1].append(i)\n",
      "                assigned = True\n",
      "                break\n",
      "        if not assigned:\n",
      "            buckets[0].append(i)\n",
      "    return buckets\n",
      "\n",
      "def sort_data_by_text(dataset, keywords):\n",
      "    buckets = [[] for i in range(len(keywords) + 1)]\n",
      "    assigned = False\n",
      "    for i in range(len(dataset)):\n",
      "        assigned = False\n",
      "        text = texts[i]\n",
      "        for k, keyword in enumerate(keywords):\n",
      "            if keyword in text:\n",
      "                buckets[k+1].append(i)\n",
      "                assigned = True\n",
      "                break\n",
      "        if not assigned:\n",
      "            buckets[0].append(i)\n",
      "    return buckets\n",
      "\n",
      "def get_occurence_for_keyword(dataset, keyword):\n",
      "    appearances = np.zeros(len(dataset), dtype=np.int32)\n",
      "    assigned = False\n",
      "    for i in range(len(dataset)):\n",
      "        assigned = False\n",
      "        text = texts[i]\n",
      "        appearances[i] = text.count(keyword)\n",
      "    return appearances"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CATEGORIES = [         \"vietnamese\" ,   \"thai\", \"tradamerican\", \"italian\", \"mexican\", \"chinese\", \"mediterranean\"]\n",
      "colors     = [\"#c3c3c3\",   \"#6bd66f\", \"#dfa53d\",      \"#37c7ff\", \"#f92772\", \"#9951ff\", \"#4b8e84\",       \"#ff484e\"]\n",
      "\n",
      "category_buckets = sort_data(datum3, CATEGORIES)\n",
      "\n",
      "fig = plt.figure(figsize=(8,8), dpi=300)\n",
      "ax = fig.add_axes([1,1,1,1])\n",
      "\n",
      "for bucket, color in zip(category_buckets[:-1], colors[:-1]):\n",
      "    ax.scatter(datum3[bucket, 0], datum3[bucket, 1], c = color, edgecolor='none')\n",
      "\n",
      "ax.legend([\"other\"] + CATEGORIES);\n",
      "ax.axis(\"off\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CATEGORIES = [         \"vietnamese\" ,   \"thai\", \"tradamerican\", \"italian\", \"mexican\", \"chinese\", \"mediterranean\"]\n",
      "colors     = [\"#c3c3c3\",   \"#6bd66f\", \"#dfa53d\",      \"#37c7ff\", \"#f92772\", \"#9951ff\", \"#4b8e84\",       \"#ff484e\"]\n",
      "\n",
      "category_buckets = sort_data(datum3, CATEGORIES)\n",
      "\n",
      "fig = plt.figure(figsize=(8,8), dpi=300)\n",
      "ax = fig.add_axes([1,1,1,1])\n",
      "\n",
      "for bucket, color in zip(category_buckets, colors):\n",
      "    ax.scatter(datum3[bucket, 0], datum3[bucket, 1], c = color, edgecolor='none')\n",
      "\n",
      "ax.legend([\"other\"] + CATEGORIES);\n",
      "ax.set_title(\"T-sne projection of Seattle Yelp restaurants\");\n",
      "frax.axis(\"off\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import cm\n",
      "\n",
      "colors     = [\"#c3c3c3\",  \"#ff484e\"]\n",
      "\n",
      "occurences = np.log(1. + get_occurence_for_keyword(datum3, \"spicy\").astype(np.float64))\n",
      "\n",
      "fig = plt.figure(figsize=(8,8), dpi=300)\n",
      "ax = fig.add_axes([1,1,1,1])\n",
      "\n",
      "scattered_spicy = ax.scatter(datum3[:, 0], datum3[:, 1], c = occurences, edgecolor='none', cmap = cm.Blues)\n",
      "fig.colorbar(scattered_spicy)\n",
      "\n",
      "ax.set_title(\"T-sne projection of Seattle Yelp restaurants, with log occurence of word spicy\");\n",
      "ax.axis(\"off\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CATEGORIES = [         \"vietnamese\" ,   \"thai\", \"tradamerican\", \"italian\", \"mexican\", \"chinese\", \"mediterranean\"]\n",
      "colors     = [\"#c3c3c3\",   \"#6bd66f\", \"#dfa53d\",      \"#37c7ff\", \"#f92772\", \"#9951ff\", \"#4b8e84\",       \"#ff484e\"]\n",
      "\n",
      "category_buckets = sort_data(datum3, CATEGORIES)\n",
      "\n",
      "fig = plt.figure(figsize=(8,8), dpi=300)\n",
      "ax = fig.add_axes([1,1,1,1])\n",
      "\n",
      "for bucket, color in zip(category_buckets, colors):\n",
      "    ax.scatter(datum2[bucket, 0], datum2[bucket, 1], c = color, edgecolor='none')\n",
      "\n",
      "ax.legend([\"other\"] + CATEGORIES);\n",
      "ax.set_title(\"T-sne projection of Seattle Yelp restaurants\");\n",
      "ax.axis(\"off\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar_word(\"he\", metric=\"euclidean\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, dist in model.most_similar_object(1306):\n",
      "    present_restaurant(texts_data[i], text=texts[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_conditional_probs_from_index([4796, 1334])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also consider making observations that are not just negative or positive, but rather a combination:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_weighted_observations(positives, negatives, weight):\n",
      "    indices, obs_vector = observables.observation_vector(\n",
      "        positive = positives,\n",
      "        negative = negatives)\n",
      "    obs_vector *= weight\n",
      "    topn = 5\n",
      "    result_inds = list(INDEX_SET - set(indices))\n",
      "    result_probs = observables._calculate_conditional_probabilities(obs_vector, indices, remaining_indices=result_inds)\n",
      "    \n",
      "    # wondering about a particular place's probability?\n",
      "    #curious_index = nearest_name(\"Lotus Asian Kitchen & Lounge\")[1]\n",
      "    #true_curious_index = np.where(np.array(result_inds) == curious_index)[0]\n",
      "    #print(result_probs[true_curious_index][0][0])\n",
      "    top_results = np.argsort(result_probs[:,0])[::-1][:topn]\n",
      "    for i in range(topn):\n",
      "        present_restaurant(texts_data[result_inds[top_results[i]]], text=texts[result_inds[top_results[i]]])\n",
      "        print(result_probs[top_results[i],0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a=nearest_name(\"lemongrass\")\n",
      "print(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b=nearest_name('burgers')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "observables.observation_vector?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_weighted_observations([a[1]],[b[1]],np.array([1,1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}